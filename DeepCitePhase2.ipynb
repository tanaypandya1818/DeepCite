{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33865393-1634-4362-9883-723948b243f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (2.32.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tanay\\anaconda3\\envs\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.5 MB 4.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.5 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/11.5 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.2/11.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.5/11.5 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.3/11.5 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.9/11.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.2/11.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading numpy-2.2.3-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.6 MB 9.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.6 MB 6.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/12.6 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.6 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 pytz-2025.1 tqdm-4.67.1 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "!pip install requests pandas tqdm\n",
    "!pip install sentence_transformers\n",
    "!pip install --upgrade ipywidgets\n",
    "!pip install --upgrade numpy\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f56132-be1d-4af4-a4b1-5d18363386d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e49c1b-3845-409f-9934-344952a897a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "CROSSREF_BASE_URL = \"https://api.crossref.org/works\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def fetch_crossref_metadata(title, rows=1000):\n",
    "    \"\"\"Fetch metadata from CrossRef API using the paper title.\"\"\"\n",
    "    params = {\"query.title\": title, \"rows\": rows}  # Increase rows to 1000\n",
    "    response = requests.get(CROSSREF_BASE_URL, params=params, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"message\" in data and \"items\" in data[\"message\"]:\n",
    "            papers = data[\"message\"][\"items\"]\n",
    "            result = []\n",
    "            for paper in papers:\n",
    "                paper_title = paper.get(\"title\", [\"Unknown\"])[0]\n",
    "                result.append({\n",
    "                    \"title\": paper_title,\n",
    "                    \"doi\": paper.get(\"DOI\", \"N/A\"),\n",
    "                    \"citation_count\": paper.get(\"is-referenced-by-count\", 0),\n",
    "                    \"year\": paper.get(\"published-print\", {}).get(\"date-parts\", [[None]])[0][0]\n",
    "                })\n",
    "            return result  # Return the list of papers\n",
    "\n",
    "    return None  # No valid papers found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed2770e-b110-4499-97bc-57a66c39c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model (you can try other models like SPECTER for academic papers)\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def get_embeddings(papers):\n",
    "    \"\"\"Generate embeddings for the list of papers using SentenceTransformer.\"\"\"\n",
    "    titles = [paper['title'] for paper in papers]\n",
    "    embeddings = model.encode(titles, convert_to_tensor=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975a10c5-5c2a-419c-8b92-aed4e6979958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"Create and store embeddings in FAISS index.\"\"\"\n",
    "    # Convert embeddings to a NumPy array for FAISS\n",
    "    embeddings_np = np.array(embeddings.cpu().detach().numpy()).astype('float32')\n",
    "    \n",
    "    # Create the FAISS index\n",
    "    index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance (Euclidean)\n",
    "    index.add(embeddings_np)  # Add embeddings to the index\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9267ba-3fb2-4ac6-988f-7f895b45820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(query, faiss_index, papers, top_k=5):\n",
    "    \"\"\"Search query in FAISS index and retrieve the most relevant papers.\"\"\"\n",
    "    # Generate the embedding for the user query\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    query_embedding_np = np.array(query_embedding.cpu().detach().numpy()).astype('float32')\n",
    "    \n",
    "    # Perform the search in the FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding_np, top_k)\n",
    "    \n",
    "    # Retrieve the top K papers from the indices\n",
    "    recommended_papers = [papers[i] for i in indices[0]]\n",
    "    return recommended_papers, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6986f11a-5768-4076-80a9-4fe783198826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_citations_and_similarity(recommended_papers, distances):\n",
    "    \"\"\"Rank papers by citation count and semantic similarity.\"\"\"\n",
    "    ranked_papers = sorted(zip(recommended_papers, distances),\n",
    "                           key=lambda x: (x[0]['citation_count'], -x[1]), reverse=True)\n",
    "    return ranked_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37f85ac-753c-4e82-b68d-4eff3f1e8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_papers(query):\n",
    "    \"\"\"Main function to fetch, rank, and return relevant papers based on query.\"\"\"\n",
    "    # Step 1: Fetch papers from CrossRef API\n",
    "    papers = fetch_crossref_metadata(query, rows=1000)\n",
    "    if not papers:\n",
    "        return \"No relevant papers found.\"\n",
    "    \n",
    "    # Step 2: Generate embeddings for the fetched papers\n",
    "    embeddings = get_embeddings(papers)\n",
    "    \n",
    "    # Step 3: Create FAISS index with the embeddings\n",
    "    faiss_index = create_faiss_index(embeddings)\n",
    "    \n",
    "    # Step 4: Search the query in the FAISS index to get top-k relevant papers\n",
    "    recommended_papers, distances = search_query(query, faiss_index, papers, top_k=10)\n",
    "    \n",
    "    # Step 5: Rank papers based on citation count and similarity\n",
    "    ranked_papers = rank_by_citations_and_similarity(recommended_papers, distances)\n",
    "    \n",
    "    # Step 6: Format the result for output\n",
    "    ranked_results = []\n",
    "    for paper, dist in ranked_papers:\n",
    "        ranked_results.append({\n",
    "            \"Title\": paper['title'],\n",
    "            \"DOI\": paper['doi'],\n",
    "            \"Citation Count\": paper['citation_count'],\n",
    "            \"Year\": paper['year'],\n",
    "            \"Similarity Distance\": dist\n",
    "        })\n",
    "    \n",
    "    return ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80ceb2f-a166-4cfd-abf8-68ccd14a4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Study of Machine Learning in Healthcare, DOI: 10.1109/compsac.2017.164, Citation Count: 133, Year: 2017, Similarity Distance: 7.934821128845215\n",
      "Title: Appropriate use of machine learning in healthcare, DOI: 10.1016/j.ibmed.2021.100041, Citation Count: 13, Year: 2021, Similarity Distance: 7.723365783691406\n",
      "Title: Application of Machine Learning Techniques in Healthcare, DOI: 10.4018/978-1-5225-9902-9.ch015, Citation Count: 12, Year: 2020, Similarity Distance: 8.307032585144043\n",
      "Title: Use of Machine Learning in Healthcare, DOI: 10.1201/9781003322597-12, Citation Count: 0, Year: 2022, Similarity Distance: 7.713277816772461\n",
      "Title: Use of Machine Learning in Healthcare, DOI: 10.1002/9781119769293.ch13, Citation Count: 0, Year: 2022, Similarity Distance: 7.713277816772461\n",
      "Title: A Survey of Machine Learning in Healthcare, DOI: 10.1201/9781003241409-1, Citation Count: 0, Year: 2022, Similarity Distance: 7.787750244140625\n",
      "Title: Exploring the Use of Machine Learning in Healthcare, DOI: 10.2991/978-94-6463-540-9_21, Citation Count: 0, Year: 2024, Similarity Distance: 8.106351852416992\n",
      "Title: Machine Learning Techniques in Healthcare—A Survey, DOI: 10.1166/jctn.2020.9061, Citation Count: 0, Year: 2020, Similarity Distance: 8.127236366271973\n",
      "Title: APPLICATION OF MACHINE LEARNING IN HEALTHCARE, DOI: 10.36106/ijsr/2522696, Citation Count: 0, Year: 2024, Similarity Distance: 8.15632438659668\n",
      "Title: Application of Machine Learning Techniques in Healthcare, DOI: 10.4018/978-1-6684-6291-1.ch067, Citation Count: 0, Year: 2022, Similarity Distance: 8.307032585144043\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Supervised Machine Learning usage in Healthcare\"\n",
    "ranked_papers = get_ranked_papers(query)\n",
    "for paper in ranked_papers:\n",
    "    print(f\"Title: {paper['Title']}, DOI: {paper['DOI']}, Citation Count: {paper['Citation Count']}, Year: {paper['Year']}, Similarity Distance: {paper['Similarity Distance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d146377-f199-4db0-ae0c-5bb31dbcf58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Supervised Machine Learning Chatbots for Perinatal Mental Healthcare, DOI: 10.1109/ichci51889.2020.00086, Citation Count: 20, Year: 2020\n",
      "Title: Supervised Machine Learning Techniques for Power Consumption Usage Level Prediction, DOI: 10.1109/csecs60003.2023.10428312, Citation Count: 0, Year: 2023\n",
      "Title: Prognosis of Supervised Machine Learning Algorithms in Healthcare Sector, DOI: 10.1109/rteict52294.2021.9573665, Citation Count: 1, Year: 2021\n",
      "Title: Machine Learning – Supervised Learning, DOI: 10.1201/9780429326813-6, Citation Count: 0, Year: 2019\n",
      "Title: Automatic emotion recognition in healthcare data using supervised machine learning, DOI: 10.7717/peerj-cs.751, Citation Count: 14, Year: None\n",
      "Title: Supervised learning (machine learning), DOI: 10.53347/rid-56096, Citation Count: 0, Year: None\n",
      "Title: Supervised Machine Learning, DOI: 10.1007/978-981-97-0217-6_8, Citation Count: 0, Year: 2024\n",
      "Title: Supervised machine learning, DOI: 10.1007/978-3-662-67882-4_6, Citation Count: 1, Year: 2023\n",
      "Title: Supervised Machine Learning, DOI: 10.1007/978-3-030-74394-9_9, Citation Count: 2, Year: 2021\n",
      "Title: Supervised (Machine) Learning, DOI: 10.1007/978-3-540-47648-1_5582, Citation Count: 1, Year: None\n"
     ]
    }
   ],
   "source": [
    "# Fetch papers from CrossRef (this step is independent of ranking)\n",
    "papers = fetch_crossref_metadata(query, rows=1000)\n",
    "\n",
    "# Print the top 10 papers fetched from CrossRef\n",
    "top_10_papers_from_crossref = papers[:10]\n",
    "for paper in top_10_papers_from_crossref:\n",
    "    print(f\"Title: {paper['title']}, DOI: {paper['doi']}, Citation Count: {paper['citation_count']}, Year: {paper['year']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
