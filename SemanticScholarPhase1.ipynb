{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "SEMANTIC_SCHOLAR_BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search/bulk\"\n",
    "HEADERS = {\"User-Agent\": \"DeepCite/1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers saved to top_1000_papers.json\n"
     ]
    }
   ],
   "source": [
    "def fetch_papers_by_keywords(keywords, fields=\"title,abstract,url,year,citationCount\", limit=1000):\n",
    "    papers = []\n",
    "    offset = 0\n",
    "    while len(papers) < limit:\n",
    "        params = {\n",
    "            \"query\": keywords,\n",
    "            \"fields\": fields,\n",
    "            \"limit\": min(limit - len(papers), 100),  #Fetch in batches of 100\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        response = requests.get(SEMANTIC_SCHOLAR_BASE_URL, headers=HEADERS, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            new_papers = data.get(\"data\", [])\n",
    "            papers.extend(new_papers)\n",
    "            offset += 100  # Move the offset to get the next set of papers\n",
    "        else:\n",
    "            print(f\"Failed to fetch data, status code: {response.status_code}\")\n",
    "            break\n",
    "    return papers[:limit]\n",
    "\n",
    "def save_papers_to_json(query, filename=\"papers.json\"):\n",
    "    papers = fetch_papers_by_keywords(query)\n",
    "    \n",
    "    if not papers:\n",
    "        print(\"No papers retrieved.\")\n",
    "        return\n",
    "    \n",
    "    # Format data to only include necessary fields\n",
    "    formatted_papers = []\n",
    "    for paper in papers:\n",
    "        formatted_papers.append({\n",
    "            \"title\": paper.get(\"title\", \"No title available\"),\n",
    "            \"abstract\": paper.get(\"abstract\", \"No abstract available\"),\n",
    "            \"year\": paper.get(\"year\", \"Unknown\"),\n",
    "            \"url\": paper.get(\"url\", \"Unknown\"),\n",
    "            \"citation_count\": paper.get(\"citationCount\", 0)\n",
    "        })\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(formatted_papers, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "query = \"Supervised machine learning use in healthcare\"\n",
    "save_papers_to_json(query, \"top_1000_papers.json\")\n",
    "\n",
    "print(\"Papers saved to top_1000_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers_by_keywords(keywords, fields=None, limit=10):\n",
    "    \n",
    "    papers = []\n",
    "    offset = 0\n",
    "    while len(papers) < limit:\n",
    "        params = {\n",
    "            \"query\": keywords,\n",
    "            \"limit\": min(limit - len(papers), 100),\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        if fields:\n",
    "            params[\"fields\"] = fields  # only add fields if specified\n",
    "\n",
    "        response = requests.get(SEMANTIC_SCHOLAR_BASE_URL, headers=HEADERS, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Print only the first paper for inspection\n",
    "            if data.get(\"data\"):\n",
    "                print(json.dumps(data[\"data\"][0], indent=4))\n",
    "                # Optional: print keys only if you want to see the field names\n",
    "                print(\"Fields available:\", list(data[\"data\"][0].keys()))\n",
    "            \n",
    "            new_papers = data.get(\"data\", [])\n",
    "            papers.extend(new_papers)\n",
    "            offset += 100\n",
    "        else:\n",
    "            print(f\"Failed to fetch data, status code: {response.status_code}\")\n",
    "            break\n",
    "    return papers[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 papers so far...\n",
      "Fetched 200 papers so far...\n",
      "Fetched 300 papers so far...\n",
      "Fetched 400 papers so far...\n",
      "Fetched 500 papers so far...\n",
      "Fetched 600 papers so far...\n",
      "Fetched 700 papers so far...\n",
      "Fetched 800 papers so far...\n",
      "Fetched 900 papers so far...\n",
      "Fetched 1000 papers so far...\n",
      "Saved 1000 papers to 'papers.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "SEMANTIC_SCHOLAR_BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "\n",
    "def fetch_and_save_papers(keywords, fields=None, limit=1000, output_file=\"papers.json\"):\n",
    "    papers = []\n",
    "    offset = 0\n",
    "\n",
    "    while len(papers) < limit:\n",
    "        params = {\n",
    "            \"query\": keywords,\n",
    "            \"limit\": min(limit - len(papers), 100),  # API max is 100 per request\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        if fields:\n",
    "            params[\"fields\"] = fields\n",
    "\n",
    "        response = requests.get(SEMANTIC_SCHOLAR_BASE_URL, headers=HEADERS, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            new_papers = data.get(\"data\", [])\n",
    "            if not new_papers:\n",
    "                print(\"No more papers found.\")\n",
    "                break\n",
    "\n",
    "            papers.extend(new_papers)\n",
    "            offset += 100\n",
    "            print(f\"Fetched {len(papers)} papers so far...\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data, status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(papers[:limit], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Saved {len(papers[:limit])} papers to '{output_file}'.\")\n",
    "\n",
    "\n",
    "\n",
    "fetch_and_save_papers(\n",
    "    keywords=\"Supervised machine learning use in healthcare\",\n",
    "    fields=\"title,abstract,year,url,citationCount,influentialCitationCount,authors\",\n",
    "    limit=1000,\n",
    "    output_file = \"papers.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
