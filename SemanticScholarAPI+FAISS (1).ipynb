{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ccac9d-b333-4f79-abdd-3a8e48d51343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Updated API Endpoint for Semantic Scholar\n",
    "SEMANTIC_SCHOLAR_BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search/bulk\"\n",
    "HEADERS = {\"User-Agent\": \"DeepCite/1.0\"}\n",
    "\n",
    "# Function to fetch papers from Semantic Scholar API based on a query\n",
    "def fetch_papers_by_keywords(keywords, fields=\"title,abstract,url,year,citationCount\", limit=1000):\n",
    "    papers = []\n",
    "    offset = 0\n",
    "    while len(papers) < limit:\n",
    "        params = {\n",
    "            \"query\": keywords,\n",
    "            \"fields\": fields,\n",
    "            \"limit\": min(limit - len(papers), 100),  # Fetch in batches of 100\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        response = requests.get(SEMANTIC_SCHOLAR_BASE_URL, headers=HEADERS, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            new_papers = data.get(\"data\", [])\n",
    "            papers.extend(new_papers)\n",
    "            offset += 100  # Move the offset to get the next set of papers\n",
    "        else:\n",
    "            print(f\"Failed to fetch data, status code: {response.status_code}\")\n",
    "            break\n",
    "    return papers[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ae9e6a-5383-4d46-857d-e17173945b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b65ea21b494bd4b7dd2210c65cfb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanay\\anaconda3\\envs\\python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tanay\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae25691e7c9e4db48e723895bad01605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5936f5d8c77a455aa55ecccbe9c0666b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dff03623de4bc7901ec0235aab08da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e375e6291284c2c816be2779f3bea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08220ec427a42e68caa67a5989008fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62671db4f18e41b9840642e7fdc976e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9fadd55a2740cf840efe558be94f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4b50d2605e4886aa3d5a4c2a9b2f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d1c57ae5a64383b040adac80d7057b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4edf7fc4d2487ab43ea1150e50cae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def get_embeddings(papers):\n",
    "    \"\"\"Generate embeddings for the list of papers using SentenceTransformer.\"\"\"\n",
    "    titles_and_abstracts = []\n",
    "    for paper in papers:\n",
    "        title = paper.get('title', '')  # Default to empty string if title is missing\n",
    "        abstract = paper.get('abstract', '')  # Default to empty string if abstract is missing\n",
    "        title = title if title else ''  # Ensure title is a string\n",
    "        abstract = abstract if abstract else ''  # Ensure abstract is a string\n",
    "        titles_and_abstracts.append(title + \" \" + abstract)\n",
    "    embeddings = model.encode(titles_and_abstracts, convert_to_tensor=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916e9f71-28dc-4dd3-815f-cd64b2d66f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "# Function to create and store embeddings in FAISS index\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"Create and store embeddings in FAISS index.\"\"\"\n",
    "    # Convert embeddings to a NumPy array for FAISS\n",
    "    embeddings_np = np.array(embeddings.cpu().detach().numpy()).astype('float32')\n",
    "    \n",
    "    # Create the FAISS index (using L2 distance, Euclidean)\n",
    "    index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "    index.add(embeddings_np)  # Add embeddings to the index\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab340365-6f2d-4249-a549-16ea8b280482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search a query in FAISS index and retrieve the most relevant papers\n",
    "def search_query(query, faiss_index, papers, top_k=5):\n",
    "    \"\"\"Search query in FAISS index and retrieve the most relevant papers.\"\"\"\n",
    "    # Generate the embedding for the user query\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    query_embedding_np = np.array(query_embedding.cpu().detach().numpy()).astype('float32')\n",
    "    \n",
    "    # Perform the search in the FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding_np, top_k)\n",
    "    \n",
    "    # Retrieve the top K papers from the indices\n",
    "    recommended_papers = [papers[i] for i in indices[0]]\n",
    "    return recommended_papers, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1febcc13-8621-4051-8ba0-07e315d09a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rank papers by citation count and similarity\n",
    "def rank_by_citations_and_similarity(papers, distances):\n",
    "    \"\"\"Rank papers by citation count and similarity distance.\"\"\"\n",
    "    papers_with_distance = list(zip(papers, distances))\n",
    "    \n",
    "    # Sort by citation count first (descending), then by similarity (ascending)\n",
    "    papers_with_distance.sort(key=lambda x: (-x[0]['citationCount'], x[1]))\n",
    "    \n",
    "    return papers_with_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6003738-56e5-4cc4-88f8-a316b70020af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_weighted_score(papers, distances, weights=(0.5, 0.3, 0.2)):\n",
    "    \"\"\"Rank papers using a weighted score: relevance, citations, and recency.\"\"\"\n",
    "    relevance_weight, citation_weight, recency_weight = weights\n",
    "    \n",
    "    # Convert FAISS distances to similarity (smaller distance = higher similarity)\n",
    "    max_dist = max(distances) + 1e-5  # avoid division by zero\n",
    "    similarity_scores = [1 - (d / max_dist) for d in distances]\n",
    "\n",
    "    # Normalize citation counts and years\n",
    "    max_citation = max(paper['citationCount'] for paper in papers) + 1e-5\n",
    "    max_year = max(paper['year'] for paper in papers) + 1e-5\n",
    "    min_year = min(paper['year'] for paper in papers)\n",
    "\n",
    "    ranked_list = []\n",
    "    for paper, sim in zip(papers, similarity_scores):\n",
    "        norm_citation = paper['citationCount'] / max_citation\n",
    "        norm_recency = (paper['year'] - min_year) / (max_year - min_year)\n",
    "\n",
    "        final_score = (\n",
    "            relevance_weight * sim +\n",
    "            citation_weight * norm_citation +\n",
    "            recency_weight * norm_recency\n",
    "        )\n",
    "\n",
    "        ranked_list.append({\n",
    "            \"Title\": paper['title'],\n",
    "            \"DOI\": paper.get('url', \"N/A\"),\n",
    "            \"Citation Count\": paper['citationCount'],\n",
    "            \"Year\": paper['year'],\n",
    "            \"Similarity Score\": round(sim, 4),\n",
    "            \"Final Score\": round(final_score, 4)\n",
    "        })\n",
    "\n",
    "    # Sort by final weighted score in descending order\n",
    "    ranked_list.sort(key=lambda x: -x['Final Score'])\n",
    "\n",
    "    return ranked_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "936eba9d-4270-4e25-a1f5-036261be52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to fetch, rank, and return relevant papers based on a query\n",
    "def get_ranked_papers(query, weights=(0.5, 0.3, 0.2)):\n",
    "    \"\"\"Main function to fetch, rank, and return relevant papers based on query.\"\"\"\n",
    "    # Step 1: Fetch papers from Semantic Scholar API\n",
    "    papers = fetch_papers_by_keywords(query, limit=1000)\n",
    "    if not papers:\n",
    "        return \"No relevant papers found.\"\n",
    "    \n",
    "    # Step 2: Generate embeddings for the fetched papers (using title and abstract)\n",
    "    embeddings = get_embeddings(papers)\n",
    "    \n",
    "    # Step 3: Create FAISS index with the embeddings\n",
    "    faiss_index = create_faiss_index(embeddings)\n",
    "    \n",
    "    # Step 4: Search the query in the FAISS index to get top-k relevant papers\n",
    "    recommended_papers, distances = search_query(query, faiss_index, papers, top_k=10)\n",
    "    \n",
    "    # Step 5: Rank papers based on citation count and similarity\n",
    "    # ranked_papers = rank_by_citations_and_similarity(recommended_papers, distances)\n",
    "    ranked_papers = rank_by_weighted_score(recommended_papers, distances, weights=weights)\n",
    "    \n",
    "    # # Step 6: Format the result for output\n",
    "    # ranked_results = []\n",
    "    # for paper, dist in ranked_papers:\n",
    "    #     ranked_results.append({\n",
    "    #         \"Title\": paper['title'],\n",
    "    #         \"DOI\": paper.get('url', \"N/A\"),  # Semantic Scholar doesn't always return DOI\n",
    "    #         \"Citation Count\": paper['citationCount'],\n",
    "    #         \"Year\": paper['year'],\n",
    "    #         \"Similarity Distance\": dist\n",
    "    #     })\n",
    "    \n",
    "    return ranked_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18b154d-8809-4bd1-b03c-3ae57994fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'APPLICATION OF MACHINE LEARNING IN HEALTHCARE', 'DOI': 'https://www.semanticscholar.org/paper/3561b270bbe783369b25bf048edc739e339340d2', 'Citation Count': 1, 'Year': 2024, 'Similarity Score': np.float32(0.5399), 'Final Score': np.float32(0.4731)}\n",
      "{'Title': 'Application of Machine Learning in Healthcare: An Analysis', 'DOI': 'https://www.semanticscholar.org/paper/4b4eb05c5ec977105fa95f90cb44baa6a4f7aa60', 'Citation Count': 5, 'Year': 2022, 'Similarity Score': np.float32(0.4429), 'Final Score': np.float32(0.3569)}\n",
      "{'Title': 'Supervised machine learning tools: a tutorial for clinicians', 'DOI': 'https://www.semanticscholar.org/paper/64e76a22692fa6f5761a70adbc58f44e9078520e', 'Citation Count': 97, 'Year': 2020, 'Similarity Score': np.float32(0.0), 'Final Score': np.float32(0.34)}\n",
      "{'Title': 'Machine Learning in Healthcare Data Analysis: A Survey', 'DOI': 'https://www.semanticscholar.org/paper/1d8a78f3f740a01905e925aeb7937df010dfa1df', 'Citation Count': 70, 'Year': 2019, 'Similarity Score': np.float32(0.246), 'Final Score': np.float32(0.3395)}\n",
      "{'Title': 'Artificial Intelligence and Machine Learning in Healthcare', 'DOI': 'https://www.semanticscholar.org/paper/276b72329076a2aedb552f310bb5bbd5168a9a0f', 'Citation Count': 16, 'Year': 2023, 'Similarity Score': np.float32(0.2217), 'Final Score': np.float32(0.3203)}\n",
      "{'Title': 'Machine learning applied to healthcare: a conceptual review', 'DOI': 'https://www.semanticscholar.org/paper/40620a51a19bb0cb61a27f920f3f9235c0b44b6b', 'Citation Count': 8, 'Year': 2022, 'Similarity Score': np.float32(0.3164), 'Final Score': np.float32(0.3029)}\n",
      "{'Title': 'Prognosis of Supervised Machine Learning Algorithms in Healthcare Sector', 'DOI': 'https://www.semanticscholar.org/paper/0ae29fc8f0d8cdc4cc8a08d9e7c05286ce1ea09b', 'Citation Count': 4, 'Year': 2021, 'Similarity Score': np.float32(0.2728), 'Final Score': np.float32(0.2288)}\n",
      "{'Title': 'Transforming Healthcare: Harnessing the Power of Machine Learning for Disease Diagnosis and Management', 'DOI': 'https://www.semanticscholar.org/paper/1f28d22ef67d70e1b6ac5ce7bf1b7a36316ec6f8', 'Citation Count': 0, 'Year': 2024, 'Similarity Score': np.float32(0.0071), 'Final Score': np.float32(0.2035)}\n",
      "{'Title': 'Leveraging machine learning and AI in healthcare: A paradigm shifts from the traditional approaches', 'DOI': 'https://www.semanticscholar.org/paper/01461f796f63e0529d294b1e7e8155708e883ecc', 'Citation Count': 0, 'Year': 2023, 'Similarity Score': np.float32(0.0679), 'Final Score': np.float32(0.194)}\n",
      "{'Title': 'Applications of Machine Learning in Healthcare Data Analysis', 'DOI': 'https://www.semanticscholar.org/paper/8c5721c5a7228ae56e40a60c7a8d25ef9268cbe7', 'Citation Count': 1, 'Year': 2020, 'Similarity Score': np.float32(0.1992), 'Final Score': np.float32(0.1427)}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Supervised Machine Learning in Healthcare\"\n",
    "ranked_papers = get_ranked_papers(query)\n",
    "\n",
    "# Output the ranked papers\n",
    "for paper in ranked_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe93621-5d87-4590-8e76-f64a345161ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
